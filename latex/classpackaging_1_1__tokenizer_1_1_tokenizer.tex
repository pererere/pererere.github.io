\doxysection{packaging.\+\_\+tokenizer.\+Tokenizer Class Reference}
\hypertarget{classpackaging_1_1__tokenizer_1_1_tokenizer}{}\label{classpackaging_1_1__tokenizer_1_1_tokenizer}\index{packaging.\_tokenizer.Tokenizer@{packaging.\_tokenizer.Tokenizer}}
\doxysubsubsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
\Hypertarget{classpackaging_1_1__tokenizer_1_1_tokenizer_afe12a2c48af1543c5128d76258a15eca}\label{classpackaging_1_1__tokenizer_1_1_tokenizer_afe12a2c48af1543c5128d76258a15eca} 
None {\bfseries \+\_\+\+\_\+init\+\_\+\+\_\+} (self, str source, \texorpdfstring{$\ast$}{*}, "{}Dict\mbox{[}str, Union\mbox{[}str, re.\+Pattern\mbox{[}str\mbox{]}\mbox{]}\mbox{]}"{} rules)
\item 
None \mbox{\hyperlink{classpackaging_1_1__tokenizer_1_1_tokenizer_aff74eaccaae690982aaaba5cb9a4d051}{consume}} (self, str name)
\item 
bool \mbox{\hyperlink{classpackaging_1_1__tokenizer_1_1_tokenizer_a6c4529c0615ee80330bf28db87789ab6}{check}} (self, str name, \texorpdfstring{$\ast$}{*}, bool peek=False)
\item 
\mbox{\hyperlink{classpackaging_1_1__tokenizer_1_1_token}{Token}} \mbox{\hyperlink{classpackaging_1_1__tokenizer_1_1_tokenizer_a151b21b99f387eddd422cd0dcf2064dd}{expect}} (self, str name, \texorpdfstring{$\ast$}{*}, str expected)
\item 
\mbox{\hyperlink{classpackaging_1_1__tokenizer_1_1_token}{Token}} \mbox{\hyperlink{classpackaging_1_1__tokenizer_1_1_tokenizer_a0338e1186dd1574082f047fdff9b3b2d}{read}} (self)
\item 
No\+Return \mbox{\hyperlink{classpackaging_1_1__tokenizer_1_1_tokenizer_a6f5877849049e0d1ca0c490e58197893}{raise\+\_\+syntax\+\_\+error}} (self, str message, \texorpdfstring{$\ast$}{*}, Optional\mbox{[}int\mbox{]} span\+\_\+start=None, Optional\mbox{[}int\mbox{]} span\+\_\+end=None)
\item 
\Hypertarget{classpackaging_1_1__tokenizer_1_1_tokenizer_a27f8cfa7a5ed91e0cba1923b9b18da3d}\label{classpackaging_1_1__tokenizer_1_1_tokenizer_a27f8cfa7a5ed91e0cba1923b9b18da3d} 
Iterator\mbox{[}None\mbox{]} {\bfseries enclosing\+\_\+tokens} (self, str open\+\_\+token, str close\+\_\+token, \texorpdfstring{$\ast$}{*}, str around)
\end{DoxyCompactItemize}
\doxysubsubsection*{Public Attributes}
\begin{DoxyCompactItemize}
\item 
\Hypertarget{classpackaging_1_1__tokenizer_1_1_tokenizer_ac59ba1bb77d052cf509b55868fd8a8ac}\label{classpackaging_1_1__tokenizer_1_1_tokenizer_ac59ba1bb77d052cf509b55868fd8a8ac} 
{\bfseries source} = source
\item 
dict \mbox{\hyperlink{classpackaging_1_1__tokenizer_1_1_tokenizer_a37276fa7c417730f020c9838b9ad431c}{rules}}
\item 
\Hypertarget{classpackaging_1_1__tokenizer_1_1_tokenizer_abbd0339d56bbd614cfedf2f44ea14025}\label{classpackaging_1_1__tokenizer_1_1_tokenizer_abbd0339d56bbd614cfedf2f44ea14025} 
Optional\mbox{[}\mbox{\hyperlink{classpackaging_1_1__tokenizer_1_1_token}{Token}}\mbox{]} {\bfseries next\+\_\+token} = None
\item 
\Hypertarget{classpackaging_1_1__tokenizer_1_1_tokenizer_a1ce175ba4f8e8fa56c5d564bccf80e83}\label{classpackaging_1_1__tokenizer_1_1_tokenizer_a1ce175ba4f8e8fa56c5d564bccf80e83} 
int {\bfseries position} = 0
\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
\begin{DoxyVerb}Context-sensitive token parsing.

Provides methods to examine the input stream to check whether the next token
matches.
\end{DoxyVerb}
 

\doxysubsection{Member Function Documentation}
\Hypertarget{classpackaging_1_1__tokenizer_1_1_tokenizer_a6c4529c0615ee80330bf28db87789ab6}\index{packaging.\_tokenizer.Tokenizer@{packaging.\_tokenizer.Tokenizer}!check@{check}}
\index{check@{check}!packaging.\_tokenizer.Tokenizer@{packaging.\_tokenizer.Tokenizer}}
\doxysubsubsection{\texorpdfstring{check()}{check()}}
{\footnotesize\ttfamily \label{classpackaging_1_1__tokenizer_1_1_tokenizer_a6c4529c0615ee80330bf28db87789ab6} 
 bool packaging.\+\_\+tokenizer.\+Tokenizer.\+check (\begin{DoxyParamCaption}\item[{}]{self}{, }\item[{str}]{name}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{bool }]{peek}{ = {\ttfamily False}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Check whether the next token has the provided name.

By default, if the check succeeds, the token *must* be read before
another check. If `peek` is set to `True`, the token is not loaded and
would need to be checked again.
\end{DoxyVerb}
 \Hypertarget{classpackaging_1_1__tokenizer_1_1_tokenizer_aff74eaccaae690982aaaba5cb9a4d051}\index{packaging.\_tokenizer.Tokenizer@{packaging.\_tokenizer.Tokenizer}!consume@{consume}}
\index{consume@{consume}!packaging.\_tokenizer.Tokenizer@{packaging.\_tokenizer.Tokenizer}}
\doxysubsubsection{\texorpdfstring{consume()}{consume()}}
{\footnotesize\ttfamily \label{classpackaging_1_1__tokenizer_1_1_tokenizer_aff74eaccaae690982aaaba5cb9a4d051} 
 None packaging.\+\_\+tokenizer.\+Tokenizer.\+consume (\begin{DoxyParamCaption}\item[{}]{self}{, }\item[{str}]{name}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Move beyond provided token name, if at current position.\end{DoxyVerb}
 \Hypertarget{classpackaging_1_1__tokenizer_1_1_tokenizer_a151b21b99f387eddd422cd0dcf2064dd}\index{packaging.\_tokenizer.Tokenizer@{packaging.\_tokenizer.Tokenizer}!expect@{expect}}
\index{expect@{expect}!packaging.\_tokenizer.Tokenizer@{packaging.\_tokenizer.Tokenizer}}
\doxysubsubsection{\texorpdfstring{expect()}{expect()}}
{\footnotesize\ttfamily \label{classpackaging_1_1__tokenizer_1_1_tokenizer_a151b21b99f387eddd422cd0dcf2064dd} 
 \mbox{\hyperlink{classpackaging_1_1__tokenizer_1_1_token}{Token}} packaging.\+\_\+tokenizer.\+Tokenizer.\+expect (\begin{DoxyParamCaption}\item[{}]{self}{, }\item[{str}]{name}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{str}]{expected}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Expect a certain token name next, failing with a syntax error otherwise.

The token is *not* read.
\end{DoxyVerb}
 \Hypertarget{classpackaging_1_1__tokenizer_1_1_tokenizer_a6f5877849049e0d1ca0c490e58197893}\index{packaging.\_tokenizer.Tokenizer@{packaging.\_tokenizer.Tokenizer}!raise\_syntax\_error@{raise\_syntax\_error}}
\index{raise\_syntax\_error@{raise\_syntax\_error}!packaging.\_tokenizer.Tokenizer@{packaging.\_tokenizer.Tokenizer}}
\doxysubsubsection{\texorpdfstring{raise\_syntax\_error()}{raise\_syntax\_error()}}
{\footnotesize\ttfamily \label{classpackaging_1_1__tokenizer_1_1_tokenizer_a6f5877849049e0d1ca0c490e58197893} 
 No\+Return packaging.\+\_\+tokenizer.\+Tokenizer.\+raise\+\_\+syntax\+\_\+error (\begin{DoxyParamCaption}\item[{}]{self}{, }\item[{str}]{message}{, }\item[{\texorpdfstring{$\ast$}{*}}]{}{, }\item[{Optional\mbox{[}int\mbox{]} }]{span\+\_\+start}{ = {\ttfamily None}, }\item[{Optional\mbox{[}int\mbox{]} }]{span\+\_\+end}{ = {\ttfamily None}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Raise ParserSyntaxError at the given position.\end{DoxyVerb}
 \Hypertarget{classpackaging_1_1__tokenizer_1_1_tokenizer_a0338e1186dd1574082f047fdff9b3b2d}\index{packaging.\_tokenizer.Tokenizer@{packaging.\_tokenizer.Tokenizer}!read@{read}}
\index{read@{read}!packaging.\_tokenizer.Tokenizer@{packaging.\_tokenizer.Tokenizer}}
\doxysubsubsection{\texorpdfstring{read()}{read()}}
{\footnotesize\ttfamily \label{classpackaging_1_1__tokenizer_1_1_tokenizer_a0338e1186dd1574082f047fdff9b3b2d} 
 \mbox{\hyperlink{classpackaging_1_1__tokenizer_1_1_token}{Token}} packaging.\+\_\+tokenizer.\+Tokenizer.\+read (\begin{DoxyParamCaption}\item[{}]{self}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Consume the next token and return it.\end{DoxyVerb}
 

\doxysubsection{Member Data Documentation}
\Hypertarget{classpackaging_1_1__tokenizer_1_1_tokenizer_a37276fa7c417730f020c9838b9ad431c}\index{packaging.\_tokenizer.Tokenizer@{packaging.\_tokenizer.Tokenizer}!rules@{rules}}
\index{rules@{rules}!packaging.\_tokenizer.Tokenizer@{packaging.\_tokenizer.Tokenizer}}
\doxysubsubsection{\texorpdfstring{rules}{rules}}
{\footnotesize\ttfamily \label{classpackaging_1_1__tokenizer_1_1_tokenizer_a37276fa7c417730f020c9838b9ad431c} 
packaging.\+\_\+tokenizer.\+Tokenizer.\+rules}

{\bfseries Initial value\+:}
\begin{DoxyCode}{0}
\DoxyCodeLine{=\ \ \{}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \ \ \ \ name:\ re.compile(pattern)\ \textcolor{keywordflow}{for}\ name,\ pattern\ \textcolor{keywordflow}{in}\ rules.items()}
\DoxyCodeLine{\ \ \ \ \ \ \ \ \}}

\end{DoxyCode}


The documentation for this class was generated from the following file\+:\begin{DoxyCompactItemize}
\item 
D\+:/\+GIT/\+Food\+Link/foodlink.\+client/node\+\_\+modules/node-\/gyp/gyp/pylib/packaging/\+\_\+tokenizer.\+py\end{DoxyCompactItemize}
